{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56275a7d-ab38-47fc-987f-dc1e39026a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## install apache ticka with docker \n",
    "!docker pull apache/tika\n",
    "!docker run -d -p 0.0.0.0:9998:9998 apache/tika\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa9237-6f46-4e5e-9c2b-9d4163c04aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://192.168.2.97:9998/tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4024ef-dd81-4dd7-9d17-954922ba1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tika\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install nltk \n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f19a9-0b67-4239-aba9-7cc99e41eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import re\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import time\n",
    "\n",
    "timer=time.time()\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM']='1' \n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\" Extract text from a given PDF file using Apache Tika. \"\"\"\n",
    "    raw = parser.from_file(pdf_path, serverEndpoint='http://192.168.2.97:9998/')\n",
    "    return raw['content']\n",
    "\n",
    "def preprocess(text):\n",
    "    return re.sub(r'\\s{2,}|\\xa0|\\x00','',text)\n",
    "\n",
    "def extract_links(text):\n",
    "    # Regex pattern to extract URLs\n",
    "    pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    urls = re.findall(pattern, text)\n",
    "    return urls\n",
    "    \n",
    "def tokenize(text):\n",
    "    import nltk \n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    nltk.download('punkt')\n",
    "    return sent_tokenize(extracted_text)\n",
    "    \n",
    "def summarize_text(text, model=0, max_length=200, with_cuda_device=-1):\n",
    "    \"\"\" Generate a summary for the provided text using Hugging Face's summarization pipeline. \"\"\"\n",
    "    \n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\",device=with_cuda_device) if model==0 \\\n",
    "        else pipeline(\"summarization\", model=\"facebook/bart-large-cnn\",device=with_cuda_device)\n",
    "    try:\n",
    "        summary = summarizer(text, max_length=max_length, min_length=100, do_sample=False )\n",
    "    except Exception as e: \n",
    "        return '' \n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "def download_file(url):\n",
    "    import requests\n",
    "    filename = url.split('/')[-1]\n",
    "    if not os.path.exists('./downloads'):\n",
    "        os.mkdir('./downloads')\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open('./downloads/'+filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "    return './downloads/'+filename\n",
    "\n",
    "# Summarize the text\n",
    "def rolling_summary(extracted_text_tk: list, **kwargs):\n",
    "    summary=[]\n",
    "    print(kwargs)\n",
    "    for i in range(1,len(extracted_text_tk)-1):\n",
    "        corpus_context='.'.join(extracted_text_tk[i-1:i])\n",
    "        if len(corpus_context)<=30 and len(corpus_context)>=1024:\n",
    "            pass\n",
    "        else: \n",
    "            summary.append(summarize_text(corpus_context\n",
    "                    , model=1, **kwargs\n",
    "                        )\n",
    "                          )\n",
    "    return summary \n",
    "\n",
    "## Get data and process \n",
    "\n",
    "# pdf_file_path=download_file('https://www.intel.com/content/dam/www/public/us/en/documents/technology-briefs/data-direct-i-o-technology-brief.pdf')    \n",
    "pdf_file_path=download_file('https://fast.dpdk.org/doc/perf/DPDK_20_11_Mellanox_NIC_performance_report.pdf')\n",
    "\n",
    "# Path to your PDF file\n",
    "# pdf_file_path = './docs/data-direct-i-o-technology-brief.pdf'\n",
    "\n",
    "# Extract text from PDF\n",
    "extracted_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "#remove useless symbols \n",
    "extracted_text = preprocess(extracted_text)\n",
    "\n",
    "#split text into tokens, as summarize_text requries<1024 symbols\n",
    "extracted_text_tk=tokenize(extracted_text)\n",
    "\n",
    "summary_1 = rolling_summary(extracted_text_tk[50:60], with_cuda_device=-1)\n",
    "summary_1=[x for x in  summary_1 if ' cnn' not in x.lower()]\n",
    "\n",
    "\n",
    "#second iteration \n",
    "summary_2 = rolling_summary(summary_1)\n",
    "summary_2=[x for x in  summary_1 if ' cnn' not in x.lower()] ## sometimes default model inejcts CNN related sentences \n",
    "\n",
    "#third iteration \n",
    "summary_3 = rolling_summary(summary_2)\n",
    "\n",
    "\n",
    "summary_3=[x for x in  summary_3 if ' cnn' not in x.lower()]\n",
    "\n",
    "print('\\n'.join(summary_3))\n",
    "\n",
    "with open(pdf_file_path+'_summary.txt', 'w+') as f:\n",
    "    f.write('\\n.'.join(summary_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ab3fe-8ab9-4911-826d-f40d61ec8c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
